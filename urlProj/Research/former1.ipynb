{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import optuna\n",
    "\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "sys.path.insert(0, '../DevCode')\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from DataManager import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_id = defaultdict(lambda: len(self.token_to_id))\n",
    "        self.id_to_token = {}\n",
    "        self.token_to_id['<PAD>'] = 0  # Padding token\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Simple tokenization by splitting on non-word characters\n",
    "        tokens = re.findall(r'\\w+|\\S', text)\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            token_id = self.token_to_id[token]\n",
    "            self.id_to_token[token_id] = token  # Store the reverse mapping\n",
    "            ids.append(token_id)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Convert a list of token IDs back to a string\n",
    "        tokens = [self.id_to_token.get(token_id, '') for token_id in token_ids]\n",
    "        return ''.join(tokens)  # Modify this line as needed for your URL format\n",
    "\n",
    "    def tokenize_column(self, series):\n",
    "        # Apply tokenization to each row in the pandas series\n",
    "        tokenized = series.apply(self.tokenize)\n",
    "        # Convert each list of tokens to a list of token IDs\n",
    "        tokenized_ids = tokenized.apply(self.convert_tokens_to_ids)\n",
    "        return tokenized_ids\n",
    "\n",
    "    def run_convert(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/malicious_phish.csv')\n",
    "tm = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_url = tm.tokenize_column(df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tokenized_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({'type':'target'},axis=1)\n",
    "df = df.drop(['url',],axis=1)\n",
    "df['sizes'] = [len(i) for i in df['tokens']]\n",
    "mean = df['sizes'].mean()\n",
    "std = df['sizes'].std()\n",
    "clipped_data = df[(df['sizes'] >= (mean - 2*std)) & (df['sizes'] <= (mean + 2*std))]\n",
    "data = df.loc[clipped_data.index]\n",
    "targets = df['target'].loc[clipped_data.index]\n",
    "\n",
    "max_length = 0\n",
    "\n",
    "for i in data['tokens'].values:\n",
    "    if len(i) > max_length:\n",
    "        max_length = len(i)\n",
    "array_data = [np.array(i) for i in data['tokens'].values]\n",
    "\n",
    "matrix_list = []\n",
    "for i in array_data:\n",
    "    new_vector = np.zeros(max_length)\n",
    "    current_vector_size = i.shape[0]\n",
    "    new_vector[:current_vector_size] = i\n",
    "    matrix_list.append(new_vector)\n",
    "\n",
    "matrix_main = np.vstack(matrix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(629456, 47)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from DataManager import SimpleTokenizer\n",
    "# df = pd.read_csv('./Data/malicious_phish.csv')\n",
    "# tm = SimpleTokenizer()\n",
    "# df['string_tokens'] = tm.tokenize_column(df['url'])\n",
    "# df['tokens'] = df['string_tokens'].apply(tm.convert_tokens_to_ids)\n",
    "# df = df.rename({'type':'target'},axis=1)\n",
    "# df = df.drop(['url', 'string_tokens'],axis=1)\n",
    "# df['sizes'] = [len(i) for i in df['tokens']]\n",
    "# mean = df['sizes'].mean()\n",
    "# std = df['sizes'].std()\n",
    "# clipped_data = df[(df['sizes'] >= (mean - 2*std)) & (df['sizes'] <= (mean + 2*std))]\n",
    "# data = df.loc[clipped_data.index]\n",
    "# targets = df['target'].loc[clipped_data.index]\n",
    "\n",
    "# max_length = 0\n",
    "\n",
    "# for i in data['tokens'].values:\n",
    "#     if len(i) > max_length:\n",
    "#         max_length = len(i)\n",
    "# array_data = [np.array(i) for i in data['tokens'].values]\n",
    "\n",
    "# matrix_list = []\n",
    "# for i in array_data:\n",
    "#     new_vector = np.zeros(max_length)\n",
    "#     current_vector_size = i.shape[0]\n",
    "#     new_vector[:current_vector_size] = i\n",
    "#     matrix_list.append(new_vector)\n",
    "\n",
    "# matrix_main = np.vstack(matrix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627328.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_main.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phishing</td>\n",
       "      <td>[1, 2, 3, 4, 5, 4, 1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>benign</td>\n",
       "      <td>[6, 4, 5, 7, 8, 7, 9, 4, 10]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>benign</td>\n",
       "      <td>[11, 4, 12, 7, 13, 7, 14, 7, 15, 4, 16]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defacement</td>\n",
       "      <td>[17, 18, 7, 7, 19, 4, 20, 2, 21, 4, 22, 7, 23,...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>defacement</td>\n",
       "      <td>[17, 18, 7, 7, 36, 2, 37, 4, 38, 7, 23, 4, 24,...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651186</th>\n",
       "      <td>phishing</td>\n",
       "      <td>[62048, 4, 3247, 4, 5, 7, 3248, 7, 3758, 7, 62...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651187</th>\n",
       "      <td>phishing</td>\n",
       "      <td>[2686, 4, 45660, 4, 5, 7, 23449, 2, 4843, 7, 1...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651188</th>\n",
       "      <td>phishing</td>\n",
       "      <td>[19, 4, 8613, 4, 5, 7, 62048, 7, 5920, 7, 4298...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651189</th>\n",
       "      <td>phishing</td>\n",
       "      <td>[187, 4, 188, 4, 12, 7, 189, 7, 627327, 327, 3...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651190</th>\n",
       "      <td>phishing</td>\n",
       "      <td>[19, 4, 15712, 4, 5, 7, 612669, 7, 627328, 7]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>629456 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target                                             tokens  sizes\n",
       "0         phishing                              [1, 2, 3, 4, 5, 4, 1]      7\n",
       "1           benign                       [6, 4, 5, 7, 8, 7, 9, 4, 10]      9\n",
       "2           benign            [11, 4, 12, 7, 13, 7, 14, 7, 15, 4, 16]     11\n",
       "3       defacement  [17, 18, 7, 7, 19, 4, 20, 2, 21, 4, 22, 7, 23,...     31\n",
       "4       defacement  [17, 18, 7, 7, 36, 2, 37, 4, 38, 7, 23, 4, 24,...     25\n",
       "...            ...                                                ...    ...\n",
       "651186    phishing  [62048, 4, 3247, 4, 5, 7, 3248, 7, 3758, 7, 62...     13\n",
       "651187    phishing  [2686, 4, 45660, 4, 5, 7, 23449, 2, 4843, 7, 1...     16\n",
       "651188    phishing  [19, 4, 8613, 4, 5, 7, 62048, 7, 5920, 7, 4298...     12\n",
       "651189    phishing  [187, 4, 188, 4, 12, 7, 189, 7, 627327, 327, 3...     12\n",
       "651190    phishing      [19, 4, 15712, 4, 5, 7, 612669, 7, 627328, 7]     10\n",
       "\n",
       "[629456 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000e+00, 2.0000e+00, 3.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [6.0000e+00, 4.0000e+00, 5.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [1.1000e+01, 4.0000e+00, 1.2000e+01, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [1.9000e+01, 4.0000e+00, 8.6130e+03, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [1.8700e+02, 4.0000e+00, 1.8800e+02, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [1.9000e+01, 4.0000e+00, 1.5712e+04, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish = np.array(data[data['target'] == 'phishing'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = matrix_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_matrix = []\n",
    "count = 0\n",
    "it = 0\n",
    "for i in fish[:-3]:\n",
    "    try:\n",
    "        fish_matrix.append(data_matrix[i])\n",
    "        count += 1\n",
    "        it = i\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_matrix = np.vstack(fish_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70789, 47)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627328.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629456"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = data_matrix[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self.generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class URLDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # Assuming the first token is the start token, and we want to predict the next token\n",
    "        # Input sequence (excluding the last token)\n",
    "        input_seq = item[:-1]\n",
    "        # Target sequence (excluding the first token)\n",
    "        target_seq = item[1:]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_dataset = URLDataset(data_matrix)\n",
    "url_dataset = URLDataset(fish_matrix)\n",
    "url_dataloader = DataLoader(url_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch, (input_seq, target_seq) in enumerate(url_dataloader):\n",
    "    input_seq = input_seq.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    input_seq = input_seq.cpu()\n",
    "    target_seq = target_seq.cpu()\n",
    "\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = input_seq.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = target_seq.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/localCyberProject2/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# ntokens = int(matrix_main.max() + 1)  # size of vocabulary; replace 'token_vocab' with your vocabulary size\n",
    "ntokens = int(fish_matrix.max() + 1)\n",
    "\n",
    "\n",
    "emsize = 32  # embedding dimension\n",
    "nhid = 32  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # the number of heads in the multiheadattention models\n",
    "dropout = 0.1  # the dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-10"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10149.232485830784\n",
      "Epoch 2, Loss: 8143.904314637184\n",
      "Epoch 3, Loss: 7879.199747681618\n",
      "Epoch 4, Loss: 7790.785867631435\n",
      "Epoch 5, Loss: 7749.44854837656\n",
      "Epoch 6, Loss: 7726.926665574312\n",
      "Epoch 7, Loss: 7712.372817873955\n",
      "Epoch 8, Loss: 7701.594541430473\n",
      "Epoch 9, Loss: 7691.053180336952\n",
      "Epoch 10, Loss: 7682.116796135902\n",
      "Epoch 11, Loss: 7672.123938918114\n",
      "Epoch 12, Loss: 7664.866341531277\n",
      "Epoch 13, Loss: 7657.399979740381\n",
      "Epoch 14, Loss: 7650.173413097858\n",
      "Epoch 15, Loss: 7646.674731492996\n",
      "Epoch 16, Loss: 7641.134645760059\n",
      "Epoch 17, Loss: 7635.3813281953335\n",
      "Epoch 18, Loss: 7625.385862708092\n",
      "Epoch 19, Loss: 7621.964706689119\n",
      "Epoch 20, Loss: 7612.969972491264\n",
      "Epoch 21, Loss: 7613.102441251278\n",
      "Epoch 22, Loss: 7606.136807799339\n",
      "Epoch 23, Loss: 7604.728419959545\n",
      "Epoch 24, Loss: 7598.545760810375\n",
      "Epoch 25, Loss: 7598.571486890316\n",
      "Epoch 26, Loss: 7598.292203843594\n",
      "Epoch 27, Loss: 7595.529361963272\n",
      "Epoch 28, Loss: 7591.979817211628\n",
      "Epoch 29, Loss: 7597.497672021389\n",
      "Epoch 30, Loss: 7597.339943885803\n",
      "Epoch 31, Loss: 7594.052661716938\n",
      "Epoch 32, Loss: 7593.399942725897\n",
      "Epoch 33, Loss: 7588.72251033783\n",
      "Epoch 34, Loss: 7592.474588215351\n",
      "Epoch 35, Loss: 7591.489813327789\n",
      "Epoch 36, Loss: 7589.988989472389\n",
      "Epoch 37, Loss: 7591.870081186295\n",
      "Epoch 38, Loss: 7589.8144745230675\n",
      "Epoch 39, Loss: 7590.348869979382\n",
      "Epoch 40, Loss: 7589.890409052372\n",
      "Epoch 41, Loss: 7585.621675670147\n",
      "Epoch 42, Loss: 7590.011239230633\n",
      "Epoch 43, Loss: 7586.390305250883\n",
      "Epoch 44, Loss: 7585.4043226242065\n",
      "Epoch 45, Loss: 7586.130188524723\n",
      "Epoch 46, Loss: 7584.126744866371\n",
      "Epoch 47, Loss: 7587.270619153976\n",
      "Epoch 48, Loss: 7588.357793241739\n",
      "Epoch 49, Loss: 7585.159619450569\n",
      "Epoch 50, Loss: 7588.211490750313\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    epoch_loss = 0.0\n",
    "    for batch, (input_seq, target_seq) in enumerate(url_dataloader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, ntokens), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        epoch_loss = total_loss / len(url_dataloader)\n",
    "        input_seq = input_seq.cpu()\n",
    "        target_seq = target_seq.cpu()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {total_loss}')\n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "    torch.save(optimizer.state_dict(), 'optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 15, Loss: 7860.097127854824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/localCyberProject2/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# ntokens = int(matrix_main.max() + 1)  # size of vocabulary; replace 'token_vocab' with your vocabulary size\n",
    "ntokens = int(fish_matrix.max() + 1)\n",
    "\n",
    "\n",
    "emsize = 32  # embedding dimension\n",
    "nhid = 32  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # the number of heads in the multiheadattention models\n",
    "dropout = 0.1  # the dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(torch.load('optimizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.module._IncompatibleKeys"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(start_token, max_len=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor([start_token], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        for _ in range(max_len - 1):\n",
    "            output = model(input_seq)\n",
    "            next_token = torch.argmax(output[-1], dim=-1).item()\n",
    "            next_token_tensor = torch.tensor([next_token], dtype=torch.long).unsqueeze(1).to(device)\n",
    "            input_seq = torch.cat([input_seq, next_token_tensor], dim=0)\n",
    "            \n",
    "            if next_token == 0:  # Replace with your end-of-sequence token\n",
    "                break\n",
    "        return input_seq.squeeze().cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "generated_url_tokens = generate_url(start_token=3)  # Replace '1' with your start token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_url_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     2, 521410,      4,      5,      7,      0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = generated_url_tokens = generate_url(start_token=2)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = generated_url_tokens = generate_url(start_token=17)\n",
    "decoded = tm.decode(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 7 0]\n",
      "br/\n",
      "\n",
      "[     2 521410      4      5      7      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "-jyuku.com////////////////\n",
      "\n",
      "[3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "icloud///////////////////\n",
      "\n",
      "[4 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      ".com//////////////////\n",
      "\n",
      "[5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "com///////////////////\n",
      "\n",
      "[6 4 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "mp3raid.com/////////////////\n",
      "\n",
      "[ 7  7  7  7  7  7 19  4  5  7  7  7  7  7  7  7  7  7  7  7]\n",
      "//////www.com///////////\n",
      "\n",
      "[8 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "music///////////////////\n",
      "\n",
      "[  9   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "krizz_kaliko-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[10  0]\n",
      "html\n",
      "\n",
      "[11  4 10  0]\n",
      "bopsecrets.html\n",
      "\n",
      "[12  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "org///////////////////\n",
      "\n",
      "[    13      2 521410      4      5      7      7      7      7      0]\n",
      "rexroth-jyuku.com////\n",
      "\n",
      "[    14      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "cr-jyuku.com///////////////\n",
      "\n",
      "[ 15  29 227  27  15  29 227  27  15  29 227  27  15  29 227  27  15  29\n",
      " 227  27]\n",
      "1&Itemid=1&Itemid=1&Itemid=1&Itemid=1&Itemid=\n",
      "\n",
      "[16  0]\n",
      "htm\n",
      "\n",
      "[   17    18     7 24821     7     0]\n",
      "http:/uploads/\n",
      "\n",
      "[   18     7 24821     7     0]\n",
      ":/uploads/\n",
      "\n",
      "[19  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "www.com/////////////////\n",
      "\n",
      "[    20      2      2 521410      4      5      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "garage--jyuku.com//////////////\n",
      "\n",
      "[21  4 10  0]\n",
      "pirenne.html\n",
      "\n",
      "[22  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "be///////////////////\n",
      "\n",
      "[23  4 10  0]\n",
      "index.html\n",
      "\n",
      "[24 25 26 27 15  4 10  0]\n",
      "php?option=1.html\n",
      "\n",
      "[25 26 27 15  4 10  0]\n",
      "?option=1.html\n",
      "\n",
      "[26 27 15  4 10  0]\n",
      "option=1.html\n",
      "\n",
      "[ 27  15  29 227  27  15  29 227  27  15  29 227  27  15  29 227  27  15\n",
      "  29 227]\n",
      "=1&Itemid=1&Itemid=1&Itemid=1&Itemid=1&Itemid\n",
      "\n",
      "[ 28  29 227  27  15  29 227  27  15   0]\n",
      "com_content&Itemid=1&Itemid=1\n",
      "\n",
      "[ 29 227  27  15  29 227  27  15  29 227  27  15  29 227  27  15  29 227\n",
      "  27  15]\n",
      "&Itemid=1&Itemid=1&Itemid=1&Itemid=1&Itemid=1\n",
      "\n",
      "[30 27 15  0]\n",
      "view=1\n",
      "\n",
      "[ 31  29 227  27  15  29 227  27  15  29 227  27  15  29 227  27  15  29\n",
      " 227  27]\n",
      "article&Itemid=1&Itemid=1&Itemid=1&Itemid=1&Itemid=\n",
      "\n",
      "[32 27 15  0]\n",
      "id=1\n",
      "\n",
      "[33  4 10  0]\n",
      "70.html\n",
      "\n",
      "[34 27 15  4 10  0]\n",
      "vsig70_0=1.html\n",
      "\n",
      "[35  7  0]\n",
      "15/\n",
      "\n",
      "[36  7  7  7  0]\n",
      "adventure///\n",
      "\n",
      "[37  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "nicaragua.com/////////////////\n",
      "\n",
      "[38  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "net///////////////////\n",
      "\n",
      "[  39   29 3411 3412 1070    4   10    0]\n",
      "com_mailto&amp;list.html\n",
      "\n",
      "[40 27 15  0]\n",
      "tmpl=1\n",
      "\n",
      "[ 41  29 227  27  15  29 227  27  15   0]\n",
      "component&Itemid=1&Itemid=1\n",
      "\n",
      "[ 42  27  15  29 227  27  15  29 227  27  15  29 227  27  15  29 227  27\n",
      "  15  29]\n",
      "link=1&Itemid=1&Itemid=1&Itemid=1&Itemid=1&\n",
      "\n",
      "[    43      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "aHR0cDovL2FkdmVudHVyZS1uaWNhcmFndWEubmV0L2luZGV4LnBocD9vcHRpb249Y29tX2NvbnRlbnQmdmlldz1hcnRpY2xlJmlkPTQ3OmFib3V0JmNhdGlkPTM2OmRlbW8tYXJ0aWNsZXMmSXRlbWlkPTU0-jyuku.com///////////////\n",
      "\n",
      "[44  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "buzzfil.com/////////////////\n",
      "\n",
      "[45  0]\n",
      "m\n",
      "\n",
      "[46  7  0]\n",
      "show/\n",
      "\n",
      "[47  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "art///////////////////\n",
      "\n",
      "[48  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "ils///////////////////\n",
      "\n",
      "[    49      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "etaient-jyuku.com///////////////\n",
      "\n",
      "[ 50   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "loin-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[51  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "de///////////////////\n",
      "\n",
      "[52  7  7  7  0]\n",
      "s///\n",
      "\n",
      "[53  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "imaginer.com/////////////////\n",
      "\n",
      "[ 54   2 620   2   2   2   2   2 620   2 620   2 620   2 620   2 620   2\n",
      "   2   2]\n",
      "que-of-----of-of-of-of-of---\n",
      "\n",
      "[    55      2      2      2 521410      4      5      7      7      0]\n",
      "le---jyuku.com//\n",
      "\n",
      "[56  7  0]\n",
      "hibou/\n",
      "\n",
      "[    57      2 521410      4      5      7      7      7      7      0]\n",
      "allait-jyuku.com////\n",
      "\n",
      "[ 58   2 620   2 620   2   2   2   2 620   2 620   2 620   2 620   2 620\n",
      "   2   2]\n",
      "faire-of-of----of-of-of-of-of--\n",
      "\n",
      "[59  4 10  0]\n",
      "ceci.html\n",
      "\n",
      "[    60      2 521410      4      5      7      7      7      7      7\n",
      "      0]\n",
      "quand-jyuku.com/////\n",
      "\n",
      "[61  4 10  0]\n",
      "filmaient.html\n",
      "\n",
      "[ 62   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "2-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[    63      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "espn.pearsonhighered.com///////////////\n",
      "\n",
      "[64  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "go.com/////////////////\n",
      "\n",
      "[    65      2      2      2 521410      4      5      7      0]\n",
      "nba---jyuku.com/\n",
      "\n",
      "[66  7  0]\n",
      "player/\n",
      "\n",
      "[67  7  0]\n",
      "_/\n",
      "\n",
      "[    68      2 521410      4      5      7      7      7      7      0]\n",
      "3457-jyuku.com////\n",
      "\n",
      "[    69      2 521410      4      5      7      7      7      7      0]\n",
      "brandon-jyuku.com////\n",
      "\n",
      "[    70      2      2      2      2      2 521410      4      5      7\n",
      "      0]\n",
      "rush-----jyuku.com/\n",
      "\n",
      "[71  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "yourbittorrent.com/////////////////\n",
      "\n",
      "[  72   27   28   29 3411 3412 1070    4   10    0]\n",
      "q=com_content&amp;list.html\n",
      "\n",
      "[    73      2      2      2      2      2 521410      4     10      0]\n",
      "anthony-----jyuku.html\n",
      "\n",
      "[    74      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "hamilton-jyuku.com///////////////\n",
      "\n",
      "[ 75   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "soulife-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[76  4 10  0]\n",
      "pashminaonline.html\n",
      "\n",
      "[    77      2      2      2 521410      4      5      7      7      0]\n",
      "pure---jyuku.com//\n",
      "\n",
      "[    78      2 521410      4      5      7      7      7      7      0]\n",
      "pashminas-jyuku.com////\n",
      "\n",
      "[79  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "allmusic.com/////////////////\n",
      "\n",
      "[80  7  0]\n",
      "album/\n",
      "\n",
      "[    81      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "crazy-jyuku.com///////////////\n",
      "\n",
      "[    82      2      2      2      2      2      2 521410      4      5\n",
      "      7      0]\n",
      "from------jyuku.com/\n",
      "\n",
      "[    83      2      2      2 521410      4      5      7      0]\n",
      "the---jyuku.com/\n",
      "\n",
      "[84  7  7  7  7  7  7  7  7  0]\n",
      "heat////////\n",
      "\n",
      "[ 85   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "r16990-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[86  4  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "corporationwiki..com////////////////\n",
      "\n",
      "[87  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "Ohio///////////////////\n",
      "\n",
      "[88  7  7  7  7  7  7  7  0]\n",
      "Columbus///////\n",
      "\n",
      "[    89      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "frank-jyuku.com///////////////\n",
      "\n",
      "[    90      2      2 521410      4      5      7      7      0]\n",
      "benson--jyuku.com//\n",
      "\n",
      "[91  4 10  0]\n",
      "P3333917.html\n",
      "\n",
      "[92 25 26 27 15  4 10  0]\n",
      "aspx?option=1.html\n",
      "\n",
      "[93  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "ikenmijnkunst.com/////////////////\n",
      "\n",
      "[94  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "nl///////////////////\n",
      "\n",
      "[95  7  0]\n",
      "exposities/\n",
      "\n",
      "[96  7  0]\n",
      "2006/\n",
      "\n",
      "[97  4  5  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "myspace.com/////////////////\n",
      "\n",
      "[98  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7]\n",
      "video///////////////////\n",
      "\n",
      "[99  7  0]\n",
      "vid/\n",
      "\n",
      "[100   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "30602581-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[   101      2      2 521410      4      5      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "lebensmittel--jyuku.com//////////////\n",
      "\n",
      "[102   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "ueberwachung.com/////////////////\n",
      "\n",
      "[103  29 227  27  15  29 227  27  15  29 227  27  15  29 227  27  15  29\n",
      " 227  27]\n",
      "aktuelles&Itemid=1&Itemid=1&Itemid=1&Itemid=1&Itemid=\n",
      "\n",
      "[104   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "szabadmunkaero.com/////////////////\n",
      "\n",
      "[105   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "hu///////////////////\n",
      "\n",
      "[   106      2 521410      4      5      7      7      7      7      7\n",
      "      0]\n",
      "cimoldal-jyuku.com/////\n",
      "\n",
      "[107  27  15   0]\n",
      "start=1\n",
      "\n",
      "[108   7   0]\n",
      "12/\n",
      "\n",
      "[109   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "larcadelcarnevale.com/////////////////\n",
      "\n",
      "[110   4  10   0]\n",
      "catalogo.html\n",
      "\n",
      "[111   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "palloncini-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[112   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "quickfacts.com/////////////////\n",
      "\n",
      "[113   2 620   2   2   2   2   2   2 620   2 620   2 620   2 620   2 620\n",
      "   2   2]\n",
      "census-of------of-of-of-of-of--\n",
      "\n",
      "[114   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "gov///////////////////\n",
      "\n",
      "[115   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "qfd///////////////////\n",
      "\n",
      "[116  25  26  27  15   4  10   0]\n",
      "maps?option=1.html\n",
      "\n",
      "[117   7   0]\n",
      "iowa_map/\n",
      "\n",
      "[   118      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "nugget.pearsonhighered.com///////////////\n",
      "\n",
      "[119   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "ca///////////////////\n",
      "\n",
      "[   120      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "ArticleDisplay.pearsonhighered.com///////////////\n",
      "\n",
      "[121   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "archive///////////////////\n",
      "\n",
      "[122   0]\n",
      "true\n",
      "\n",
      "[   123      2      2      2      2      2 521410      4      5      7\n",
      "      0]\n",
      "e-----jyuku.com/\n",
      "\n",
      "[124   4  10   0]\n",
      "1160966.html\n",
      "\n",
      "[125   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "uk///////////////////\n",
      "\n",
      "[   126      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "linkedin.pearsonhighered.com///////////////\n",
      "\n",
      "[127   7   7   7   7   7   7   7   0]\n",
      "pub///////\n",
      "\n",
      "[   128      2      2      2      2 521410      4     10      0]\n",
      "steve----jyuku.html\n",
      "\n",
      "[129   7   7   7   7   0]\n",
      "rubenstein////\n",
      "\n",
      "[130   4  10   0]\n",
      "8.html\n",
      "\n",
      "[   131      2      2      2      2      2      2      2 521410      4\n",
      "      5      7      7      7      7      7      7      7      7      7]\n",
      "718-------jyuku.com/////////\n",
      "\n",
      "[132   7   0]\n",
      "755/\n",
      "\n",
      "[133   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "vnic.com/////////////////\n",
      "\n",
      "[134   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "co.com/////////////////\n",
      "\n",
      "[   135      2    620      2      2      2      2 521410      4      5\n",
      "      7      0]\n",
      "khach-of----jyuku.com/\n",
      "\n",
      "[   136      2    620      2      2      2      2 521410      4      5\n",
      "      7      0]\n",
      "hang-of----jyuku.com/\n",
      "\n",
      "[   137      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "baseball-jyuku.com///////////////\n",
      "\n",
      "[138   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "reference.com/////////////////\n",
      "\n",
      "[139   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "players///////////////////\n",
      "\n",
      "[140   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "h///////////////////\n",
      "\n",
      "[   141      2 521410      4      5      7      7      7      7      7\n",
      "      0]\n",
      "harrige01-jyuku.com/////\n",
      "\n",
      "[142   0]\n",
      "shtml\n",
      "\n",
      "[143   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "signin///////////////////\n",
      "\n",
      "[   144      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "eby.pearsonhighered.com///////////////\n",
      "\n",
      "[145   4  10   0]\n",
      "zukruygxctzmmqi.html\n",
      "\n",
      "[146   4  10   0]\n",
      "civpro.html\n",
      "\n",
      "[147   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "za///////////////////\n",
      "\n",
      "[148   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "192.com/////////////////\n",
      "\n",
      "[149   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "atoz///////////////////\n",
      "\n",
      "[150   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "people///////////////////\n",
      "\n",
      "[   151      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "oakley.pearsonhighered.com///////////////\n",
      "\n",
      "[   152      2      2      2      2      2 521410      4      5      7\n",
      "      0]\n",
      "patrick-----jyuku.com/\n",
      "\n",
      "[   153      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "nytimes.pearsonhighered.com///////////////\n",
      "\n",
      "[154   7   0]\n",
      "1998/\n",
      "\n",
      "[155   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "03///////////////////\n",
      "\n",
      "[156   7   0]\n",
      "29/\n",
      "\n",
      "[157   4  10   0]\n",
      "style.html\n",
      "\n",
      "[   158      2 521410      4      5      7      7      7      7      7\n",
      "      0]\n",
      "cuttings-jyuku.com/////\n",
      "\n",
      "[159   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "oh///////////////////\n",
      "\n",
      "[160   2   2 620   2   2   2   2   2 620   2 620   2 620   2 620   2 620\n",
      "   2   2]\n",
      "that--of-----of-of-of-of-of--\n",
      "\n",
      "[161   4  10   0]\n",
      "brazen.html\n",
      "\n",
      "[162   7   0]\n",
      "raucous/\n",
      "\n",
      "[   163      2    620      2      2      2      2 521410      4      5\n",
      "      7      0]\n",
      "glorious-of----jyuku.com/\n",
      "\n",
      "[164   4  10   0]\n",
      "hibiscus.html\n",
      "\n",
      "[165   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2 476   2\n",
      " 476   2]\n",
      "escholarship-to-to-to-to-to-to-to-to-to-\n",
      "\n",
      "[166  25  26  27  15   7   0]\n",
      "uc?option=1/\n",
      "\n",
      "[167   7   0]\n",
      "item/\n",
      "\n",
      "[168   7   7   0]\n",
      "5xt4952c//\n",
      "\n",
      "[169   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "songfacts.com/////////////////\n",
      "\n",
      "[170   7   0]\n",
      "detail/\n",
      "\n",
      "[171   4  10   0]\n",
      "13410.html\n",
      "\n",
      "[   172      2 521410      4      5      7      7      7      7      7\n",
      "      0]\n",
      "casamanana-jyuku.com/////\n",
      "\n",
      "[173   7   0]\n",
      "education/\n",
      "\n",
      "[174   7   0]\n",
      "blba/\n",
      "\n",
      "[175   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "hollywoodlife.com/////////////////\n",
      "\n",
      "[176   7   0]\n",
      "2014/\n",
      "\n",
      "[177   7   0]\n",
      "05/\n",
      "\n",
      "[178   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "01///////////////////\n",
      "\n",
      "[   179      2      2      2      2      2      2 521410      4      5\n",
      "      7      0]\n",
      "rihanna------jyuku.com/\n",
      "\n",
      "[   180      2 521410      4      5      7      7      7      7      7\n",
      "      0]\n",
      "iheartradio-jyuku.com/////\n",
      "\n",
      "[181   7   0]\n",
      "awards/\n",
      "\n",
      "[   182      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "dress-jyuku.com///////////////\n",
      "\n",
      "[183   4  10   0]\n",
      "pics.html\n",
      "\n",
      "[184   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "marketingbyinternet.com/////////////////\n",
      "\n",
      "[185   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "mo///////////////////\n",
      "\n",
      "[186   7   7   7   0]\n",
      "e56508df639f6ce7d55c81ee3fcd5ba8///\n",
      "\n",
      "[187   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "en///////////////////\n",
      "\n",
      "[188   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "wikipedia.com/////////////////\n",
      "\n",
      "[189   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "wiki///////////////////\n",
      "\n",
      "[190   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "North_Dakota///////////////////\n",
      "\n",
      "[191   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "soaps.com/////////////////\n",
      "\n",
      "[   192      4 591904      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "sheknows.pearsonhighered.com///////////////\n",
      "\n",
      "[   193      2      2 521410      4      5      7      0]\n",
      "daysofourlives--jyuku.com/\n",
      "\n",
      "[194   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "news///////////////////\n",
      "\n",
      "[   195      2 521410      4      5      7      7      7      7      7\n",
      "      7      7      7      7      7      7      7      7      7      7]\n",
      "20259-jyuku.com///////////////\n",
      "\n",
      "[   196      2 521410      4      5      7      7      7      7      0]\n",
      "Days_Of_Our_Lives_Casts_Ruta_Lee-jyuku.com////\n",
      "\n",
      "[197   4   5   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "perfectpeople.com/////////////////\n",
      "\n",
      "[198   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7]\n",
      "celebrity///////////////////\n",
      "\n",
      "[   199      2      2 521410      4      5      7      7      7      7\n",
      "      0]\n",
      "star--jyuku.com////\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,200):\n",
    "    url = generated_url_tokens = generate_url(start_token=i)\n",
    "    decoded = tm.decode(url)\n",
    "    print(url)\n",
    "    print(decoded)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 18,  7,  7,  7,  0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http:///'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/localCyberProject2/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m  \u001b[39m# You can adjust this\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     epoch_loss \u001b[39m=\u001b[39m train(url_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m0.5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/localCyberProject2/urlProjectML/Research/former1.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "ntokens = int(matrix_main.max() + 1)  # size of vocabulary; replace 'token_vocab' with your vocabulary size\n",
    "emsize = 32  # embedding dimension\n",
    "nhid = 32  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # the number of heads in the multiheadattention models\n",
    "dropout = 0.2  # the dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch, (input_seq, target_seq) in enumerate(dataloader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, ntokens), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10  # You can adjust this\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_loss = train(url_dataloader)\n",
    "    print(f'Epoch {epoch}, Loss: {epoch_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
